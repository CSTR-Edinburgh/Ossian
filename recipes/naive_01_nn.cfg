#!/usr/bin/env python

import sys
import os
import inspect
current_dir = os.path.realpath(os.path.abspath(os.path.dirname(inspect.getfile(inspect.currentframe()))))

## for when config is still in recipes directory:
sys.path.append(current_dir + '/../scripts/')
sys.path.append(current_dir + '/../scripts/processors/')

## for after config is copied to voice.cfg:
sys.path.append(current_dir + '/../../../../scripts/')
sys.path.append(current_dir + '/../../../../scripts/processors/')

from Tokenisers import RegexTokeniser
from Phonetisers import NaivePhonetiser
from VSMTagger import VSMTagger
from FeatureExtractor import WorldExtractor
from FeatureDumper import FeatureDumper
from Aligner import StateAligner
from SKLProcessors import SKLDecisionTreePausePredictor 
from PhraseMaker import PhraseMaker
from AcousticModel import AcousticModelWorld
from NN import NNDurationPredictor, NNAcousticPredictor


import default.const as c







## ----------------------------------------------------------------
## First define a few things used later:

## Some useful Xpaths and regex:--
#CONTENT_NODES = "//token[@token_class='word'] | //token[@token_class='punctuation']"
JUNCTURE_NODES = "//token[@token_class='space'] | //token[@token_class='punctuation']"

LETTER_PATT = '[\p{L}||\p{N}||\p{M}]'
PUNC_PATT = '[\p{C}||\p{P}||\p{S}]'
SPACE_PATT = '\p{Z}'
PUNC_OR_SPACE_PATT = '[\p{Z}||\p{C}||\p{P}||\p{S}]'


tokenisation_pattern = '('+SPACE_PATT+'*'+PUNC_PATT+'*'+SPACE_PATT+'+|'+SPACE_PATT+'*'+PUNC_PATT+'+\Z)'
                                 ## modified to handle word-internal hyphens

word_vsm_dim = 10

speech_coding_config = {'order': 59, 'static_window': '1', 'delta_window': '-0.5 0.0 0.5', 'delta_delta_window': '1.0 -2.0 1.0'}



## Define various context features used by processors:

pause_predictor_features = [        
        ('response', './attribute::has_silence="yes"'), 
        ('token_is_punctuation', './attribute::token_class="punctuation"'),      
        ('since_start_utterance_in_words', "count(preceding::token[@token_class='word'])"),
        ('till_end_utterance_in_words', "count(following::token[@token_class='word'])")
]     
## add word VSM features to pause predictor:
for dim in range(1, word_vsm_dim+1):
    pause_predictor_features.append(('L_vsm_d%s'%(dim), "./preceding::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
    pause_predictor_features.append(('C_vsm_d%s'%(dim), "./attribute::vsm_d%s"%(dim)))
    pause_predictor_features.append(('R_vsm_d%s'%(dim), "./following::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))


## These can be used from either the state or phone level:
phone_and_state_contexts = [
    ## WORD LEVEL:
    ("length_left_word", "count(ancestor::token/preceding::token[@token_class='word'][1]/descendant::segment)"),
    ("length_current_word", "count(ancestor::token/descendant::segment)"),
    ("length_right_word", "count(ancestor::token/following::token[@token_class='word'][1]/descendant::segment)"),
    ("since_beginning_of_word", "count_Xs_since_start_Y('segment', 'token')"),
    ("till_end_of_word", "count_Xs_till_end_Y('segment', 'token')"),

    ## phrase LEVEL:
    ("length_l_phrase_in_words", "count(ancestor::phrase/preceding::phrase[1]/descendant::token[@token_class='word'])"),
    ("length_c_phrase_in_words", "count(ancestor::phrase/descendant::token[@token_class='word'])"),
    ("length_r_phrase_in_words", "count(ancestor::phrase/following::phrase[1]/descendant::token[@token_class='word'])"),

    ("length_l_phrase_in_segments", "count(ancestor::phrase/preceding::phrase[1]/descendant::segment)"),
    ("length_c_phrase_in_segments", "count(ancestor::phrase/descendant::segment)"),
    ("length_r_phrase_in_segments", "count(ancestor::phrase/following::phrase[1]/descendant::segment)"),

    ("since_phrase_start_in_segs", "count_Xs_since_start_Y('segment', 'phrase')"),
    ("till_phrase_end_in_segs", "count_Xs_till_end_Y('segment', 'phrase')"),

    ("since_phrase_start_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'phrase')"),
    ("till_phrase_end_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'phrase')"),

    ## SENTENCE (UTT) LEVEL
    ("since_start_sentence_in_segments", "count_Xs_since_start_Y('segment', 'utt')"),
    ("since_start_sentence_in_words", "count_Xs_since_start_Y('token[@token_class=\"word\"]', 'utt')"),
    ("since_start_sentence_in_phrases", "count_Xs_since_start_Y('phrase', 'utt')"),
    ("till_end_sentence_in_segments", "count_Xs_till_end_Y('segment', 'utt')"),
    ("till_end_sentence_in_words", "count_Xs_till_end_Y('token[@token_class=\"word\"]', 'utt')"),
    ("till_end_sentence_in_phrases", "count_Xs_till_end_Y('phrase', 'utt')"),
    ("length_sentence_in_segments", "count(ancestor::utt/descendant::segment)"),
    ("length_sentence_in_words", "count(ancestor::utt/descendant::token[@token_class='word'])"),
    ("length_sentence_in_phrases", "count(ancestor::utt/descendant::phrase)")
]

## add word VSM features:
for dim in range(1, word_vsm_dim+1):
    phone_and_state_contexts.append(('L_vsm_d%s'%(dim), "./ancestor::token/preceding::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))
    phone_and_state_contexts.append(('C_vsm_d%s'%(dim), "./ancestor::token/attribute::vsm_d%s"%(dim)))
    phone_and_state_contexts.append(('R_vsm_d%s'%(dim), "./ancestor::token/following::token[@token_class='word'][1]/attribute::vsm_d%s"%(dim)))



phone_contexts = [
        ## special named features:
        ('htk_monophone', './attribute::pronunciation'),
        ('start_time', './attribute::start'),
        ('end_time', './attribute::end'),

        ## normal features:
        ('ll_segment', 'preceding::segment[2]/attribute::pronunciation'),
        ('l_segment', 'preceding::segment[1]/attribute::pronunciation'),
        ('c_segment', './attribute::pronunciation'),
        ('r_segment', 'following::segment[1]/attribute::pronunciation'),
        ('rr_segment', 'following::segment[2]/attribute::pronunciation')      
] + phone_and_state_contexts


state_contexts = [
    ## special named features:
    ("start_time", "./attribute::start"),
    ("end_time", "./attribute::end"),
    ("htk_state", "count(./preceding-sibling::state) + 1"),
    ("htk_monophone", "./ancestor::segment/attribute::pronunciation"),

    ("ll_segment", "./ancestor::segment/preceding::segment[2]/attribute::pronunciation"),
    ("l_segment", "./ancestor::segment/preceding::segment[1]/attribute::pronunciation"),
    ("c_segment", "./ancestor::segment/attribute::pronunciation"),
    ("r_segment", "./ancestor::segment/following::segment[1]/attribute::pronunciation"),
    ("rr_segment", "./ancestor::segment/following::segment[2]/attribute::pronunciation")
] + phone_and_state_contexts

## This is a set of Xpaths for obtaining state durations:-
duration_data_contexts = [('state_%s_nframes'%(i), '(./state[%s]/attribute::end - ./state[%s]/attribute::start) div 5'%(i,i)) for i in [1,2,3,4,5]]


## ----------------------------------------------------------------
## Now, a number of utterance processors are defined:--

tokeniser = RegexTokeniser('word_splitter', target_nodes='//utt', split_attribute='text', \
                            child_node_type='token', add_terminal_tokens=True, \
                            class_patterns = [('space', '\A'+SPACE_PATT+'+\Z'), ('punctuation', '\A'+PUNC_OR_SPACE_PATT+'+\Z')], \
                            split_pattern=tokenisation_pattern)
                                                

phonetiser = NaivePhonetiser('segment_adder', target_nodes="//token", target_attribute='text', child_node_type='segment', \
                            class_attribute='token_class', output_attribute='pronunciation', word_classes = ['word'], \
                            probable_pause_classes = ['punctuation', c.TERMINAL], possible_pause_classes=['space'])

word_vector_tagger = VSMTagger('word_vector_tagger', target_nodes="//token[@token_class='word']", input_attribute='text', \
                    output_attribute_stem='vsm', norm_counts=True, svd_algorithm='randomized',  context_size=250, rank=word_vsm_dim, \
                    unseen_method=5, discretisation_method='none', tokenisation_pattern=tokenisation_pattern)

speech_feature_extractor = WorldExtractor('acoustic_feature_extractor', input_filetype='wav', output_filetype='cmp', \
                            coding_config=speech_coding_config, sample_rate=48000, alpha=0.77, mcep_order=59)

align_label_dumper = FeatureDumper(target_nodes='//segment', output_filetype='align_lab', contexts=[('segment', './attribute::pronunciation')])
 
aligner = StateAligner(target_nodes='//segment', target_attribute='pronunciation', input_label_filetype='align_lab', acoustic_feature_filetype='cmp', \
                    output_label_filetype='time_lab', silence_tag='has_silence', min_silence_duration=50, viterbi_beam_width='1000 100000 1000000', \
                    acoustic_subrecipe='standard_alignment')

pause_predictor = SKLDecisionTreePausePredictor(processor_name='pause_predictor', target_nodes=JUNCTURE_NODES, output_attribute='silence_predicted', contexts=pause_predictor_features)
        
phrase_adder = PhraseMaker(node_type_to_regroup='token', parent_node_type='phrase', \
                 attribute_with_silence='pronunciation', silence_symbol='sil')

dur_data_maker = FeatureDumper(processor_name='duration_data_maker', target_nodes='//segment', context_separators='spaces', output_filetype='dur', \
                question_file='null', contexts=duration_data_contexts, binary_output=True)

dur_label_maker = FeatureDumper(processor_name='labelmaker', target_nodes='//segment', context_separators='numbers', output_filetype='lab_dur', \
                question_file='questions_dur.hed', question_filter_threshold = 5, contexts=phone_contexts)

duration_predictor = NNDurationPredictor(processor_name='duration_predictor', question_file='questions_dur.hed', target_nodes='//segment')

dnn_label_maker = FeatureDumper(processor_name='dnn_labelmaker', target_nodes='//state', context_separators='numbers', output_filetype='lab_dnn', \
                question_file='questions_dnn.hed', question_filter_threshold=5, contexts=state_contexts)

acoustic_predictor = NNAcousticPredictor(variance_expansion=0.3, sample_rate=48000, alpha=0.77, mcep_order=59, input_label_filetype='lab_dnn')


## -----------------------------------------------------------------
## The processors are grouped for convenience into several 'stages':

text_proc = [tokeniser, phonetiser, word_vector_tagger]
alignment = [align_label_dumper, speech_feature_extractor, aligner, pause_predictor, phrase_adder, dur_data_maker]
pause_prediction = [pause_predictor, phrase_adder] 
speech_generation = [dur_label_maker, duration_predictor, dnn_label_maker, acoustic_predictor]



## ----------------------------------------------------------------
## The final part of the config specifies which stages are run in each of the modes
## "train" and "runtime" (and optionally extra, specialised, modes):

train_stages   = [text_proc, alignment,        speech_generation]

runtime_stages = [text_proc, pause_prediction, speech_generation]

