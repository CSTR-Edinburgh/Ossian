[DEFAULT]

## The DEFAULT section just gives a few global variables -- this is designed to reduce the
## number of paths you have to change when modifying this config file.

##!!! Change the following line to the top level of your copy of the Ossian code:
OSSIAN: __INSERT_PATH_TO_OSSIAN_HERE__
LANGUAGE: __INSERT_LANGUAGE_HERE__
SPEAKER: __INSERT_SPEAKER_HERE__
RECIPE: __INSERT_RECIPE_HERE__


## This line should point to the language/data/recipe combination you are working on:
TOP: %(OSSIAN)s/train/%(LANGUAGE)s/speakers/%(SPEAKER)s/%(RECIPE)s/


## spot for putting things in training -- not the final stored model:
WORKDIR: %(TOP)s/dnn_training_DUR/
DATADIR: %(TOP)s/




[Paths]

work: %(WORKDIR)s/
data: %(DATADIR)s/

plot: %(WORKDIR)s/plots

file_id_list: __INSERT_FILELIST_HERE__

log_config_file: %(OSSIAN)s/tools/merlin/egs/slt_arctic/s1/conf/logging_config.conf
log_file: %(WORKDIR)s/log/log.txt
log_path: %(WORKDIR)s/log/

## You won't need these -- just leave the placeholder paths here:
sptk     :   /this/path/does/not/exist
straight :   /this/path/does/not/exist


in_dur_dir: %(DATADIR)s/dur






[Labels]


question_file_name  : %(TOP)s/questions_dur.hed.cont
silence_pattern: ['*/THIS-STRING-DOESNT-APPEAR-IN-LABELS/*'] 
label_type: phone_align
label_align: %(TOP)s/lab_dur
add_frame_features: False
subphone_feats: none

[Extensions]

lab_ext: .lab_dur
dur_ext: .dur

[Outputs]
## This says that we are predicting 5 state durations per example (letter/phone)
dur    : 5


[Waveform]

## This won't be used -- but keep it here as a placeholder:
vocoder_type : WORLD
framelength : 2048

[Architecture]

## Adjust the number and size of hidden layers here:
hidden_layer_size  : [512, 512, 512]
hidden_layer_type  : ['TANH', 'TANH', 'TANH']


## if RNN or sequential training is used, please set sequential_training to True. For 
## use with Ossian, we will only train DNNs, so don't alter this.
sequential_training : False

## You might want to experiment with different learning rates, batch sizes, and maximum 
## number of training epochs:
learning_rate    : 0.002
batch_size       : 256
training_epochs  : 6
## set warmup_epoch to a number larger than training_epochs to effectively disable it
warmup_epoch     : 1000 

L1_regularization: 0.0
L2_regularization: 0.0
hidden_activation: tanh
output_activation: linear
warmup_momentum  : 0.0
private_l2_reg   : 0.0

[Streams]
# which feature to be used in the output
output_features      : ['dur']


[Data]
## We need to divide the files available up into train/validation/test data. We don't need 
## to do any testing, but set test_file_number to 1 to keep the tools happy. Split the remaining
## files between train and validation. Using about 5% or 10% of the data for validation is 
## pretty standard. This is how you might divide up 28 files:
train_file_number: __INSERT_NUMBER_OF_TRAINING_FILES_HERE__
valid_file_number: __INSERT_NUMBER_OF_VALIDATION_FILES_HERE__
test_file_number : __INSERT_NUMBER_OF_TEST_FILES_HERE__
#buffer size of each block of data to
buffer_size: 200000

[Utility]

plot : True

[Processes]
## For use with Ossian, just keep the first 4 set to True -- we will generate speech later 
## within Ossian itself. You can run each of the 4 steps individually if you like:
NORMLAB  : True
MAKECMP  : True
NORMCMP  : True
TRAINDNN : True
DNNGEN   : False
GENWAV   : False
CALMCD   : False


