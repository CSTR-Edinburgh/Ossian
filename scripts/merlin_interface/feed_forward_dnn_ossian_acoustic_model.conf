[DEFAULT]

## The DEFAULT section just gives a few global variables -- this is designed to reduce the
## number of paths you have to change when modifying this config file.

##!!! Change the following line to the top level of your copy of the Ossian code:
OSSIAN: __INSERT_PATH_TO_OSSIAN_HERE__
LANGUAGE: __INSERT_LANGUAGE_HERE__
SPEAKER: __INSERT_SPEAKER_HERE__
RECIPE: __INSERT_RECIPE_HERE__


## This line should point to the language/data/recipe combination you are working on:
TOP: %(OSSIAN)s/train/%(LANGUAGE)s/speakers/%(SPEAKER)s/%(RECIPE)s/

## spot for putting things in training -- not the final stored model:
WORKDIR: %(TOP)s/dnn_training_ACOUST/
DATADIR: %(TOP)s/cmp/

[Paths]

work: %(WORKDIR)s/
data: %(DATADIR)s/

plot: %(WORKDIR)s/plots

file_id_list: __INSERT_FILELIST_HERE__

log_config_file: %(OSSIAN)s/tools/merlin/egs/slt_arctic/s1/conf/logging_config.conf
log_file: %(WORKDIR)s/log/log.txt
log_path: %(WORKDIR)s/log/

## You won't need these -- just leave the placeholder paths here:
sptk     :   /this/path/does/not/exist
straight :   /this/path/does/not/exist

in_mgc_dir: %(DATADIR)s/ 
in_lf0_dir: %(DATADIR)s/
in_bap_dir: %(DATADIR)s/

[Labels]

question_file_name  : %(TOP)s/questions_dnn.hed.cont
silence_pattern: ['*-sil+*'] 
label_type: state_align
label_align: %(TOP)s/lab_dnn
add_frame_features: True
subphone_feats: full


[Extensions]

lab_ext: .lab_dnn
mgc_ext: .mgc
bap_ext: .bap
lf0_ext: .lf0

[Outputs]
## mgc, bap and lf0 need to be the same sizes as the static streams used when calling
## split_cmp.py previously; the corresponding variables starting d* are just the static
## value multiplied by 3: 
mgc    : __INSERT_MGC_DIM_HERE__
dmgc   : __INSERT_DELTA_MGC_DIM_HERE__
bap    : __INSERT_BAP_DIM_HERE__
dbap   : __INSERT_DELTA_BAP_DIM_HERE__
lf0    : __INSERT_LF0_DIM_HERE__
dlf0   : __INSERT_DELTA_LF0_DIM_HERE__


[Waveform]

## This won't be used -- but keep it here as a placeholder:
vocoder_type : WORLD
framelength : 2048

[Architecture]

## Adjust the number and size of hidden layers here:

hidden_layer_size  : [1024, 1024, 1024, 1024, 1024, 1024]
hidden_layer_type  : ['TANH', 'TANH', 'TANH', 'TANH', 'TANH', 'TANH']

## if RNN or sequential training is used, please set sequential_training to True. For 
## use with Ossian, we will only train DNNs, so don't alter this.
sequential_training : False

## You might want to experiment with different learning rates, batch sizes, and maximum 
## number of training epochs:
learning_rate    : 0.002
batch_size       : 256
training_epochs  : 12
## set warmup_epoch to a number larger than training_epochs to effectively disable it
warmup_epoch     : 1000 

L1_regularization: 0.0
L2_regularization: 0.0
hidden_activation: tanh
output_activation: linear
warmup_momentum  : 0.0
private_l2_reg   : 0.0

[Streams]
# which feature to be used in the output
output_features      : ['mgc', 'lf0', 'vuv', 'bap']


[Data]
## We need to divide the files available up into train/validation/test data. We don't need 
## to do any testing, but set test_file_number to 1 to keep the tools happy. Split the remaining
## files between train and validation. Using about 5% or 10% of the data for validation is 
## pretty standard. This is how you might divide up 28 files:
train_file_number: __INSERT_NUMBER_OF_TRAINING_FILES_HERE__
valid_file_number: __INSERT_NUMBER_OF_VALIDATION_FILES_HERE__
test_file_number : __INSERT_NUMBER_OF_TEST_FILES_HERE__
#buffer size of each block of data to
buffer_size: 100000

[Utility]

plot : True

[Processes]
## For use with Ossian, just keep the first 4 set to True -- we will generate speech later 
## within Ossian itself. You can run each of the 4 steps individually if you like:
NORMLAB  : True
MAKECMP  : True
NORMCMP  : True
TRAINDNN : True
DNNGEN   : False
GENWAV   : False
CALMCD   : False


